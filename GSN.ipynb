{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4jQLxm0TSg3X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionBasedInflationBlock(nn.Module):\n",
        "    def __init__(self, dim, spatial_layers=3, activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pretrained Spatial Layer(s)\n",
        "        self.pretrained_spatial_layers = nn.ModuleList([\n",
        "            nn.Conv3d(dim, dim, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
        "            for _ in range(spatial_layers)\n",
        "        ])\n",
        "\n",
        "        # 2D Convolution\n",
        "        self.conv2d = nn.Conv3d(dim, dim, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
        "\n",
        "        # 1D Convolution\n",
        "        self.conv1d = nn.Conv3d(dim, dim, kernel_size=(3, 1, 1), padding=(1, 0, 0))\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        # Activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'gelu':\n",
        "            self.activation = nn.GELU()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
        "\n",
        "        # Linear Projection\n",
        "        self.linear_projection = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, T, C, H, W)\n",
        "        x = x.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)\n",
        "\n",
        "        # Pretrained Spatial Layer(s)\n",
        "        for layer in self.pretrained_spatial_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # 2D Convolution\n",
        "        x = self.conv2d(x)\n",
        "\n",
        "        # Norm + activation\n",
        "        x = x.permute(0, 2, 3, 4, 1)  # (B, T, H, W, C)\n",
        "        x = self.norm1(x)\n",
        "        x = self.activation(x)\n",
        "        x = x.permute(0, 4, 1, 2, 3)  # (B, C, T, H, W)\n",
        "\n",
        "        # 1D Convolution\n",
        "        x = self.conv1d(x)\n",
        "\n",
        "        # Norm + activation\n",
        "        x = x.permute(0, 2, 3, 4, 1)  # (B, T, H, W, C)\n",
        "        x = self.norm2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Linear Projection\n",
        "        x = self.linear_projection(x)\n",
        "\n",
        "        # Return to original shape\n",
        "        x = x.permute(0, 1, 4, 2, 3)  # (B, T, C, H, W)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ItyBB1gFn00G"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBasedInflationBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=9, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.pretrained_spatial_layers = nn.ModuleList([\n",
        "            nn.Conv3d(dim, dim, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
        "            for _ in range(3)  # Assuming 3 pretrained spatial layers\n",
        "        ])\n",
        "        self.id_attention = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, batch_first=True)\n",
        "        self.linear_projection = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, T, C, H, W)\n",
        "\n",
        "        # Permute to (B, C, T, H, W) for Conv3d layers\n",
        "        x = x.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        # Apply pretrained spatial layers\n",
        "        for layer in self.pretrained_spatial_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Reshape for attention\n",
        "        B, C, T, H, W = x.shape\n",
        "        x = x.permute(0, 2, 3, 4, 1).reshape(B*T, H*W, C)\n",
        "\n",
        "        # Apply ID Attention\n",
        "        x, _ = self.id_attention(x, x, x)\n",
        "\n",
        "        # Apply Linear Projection\n",
        "        x = self.linear_projection(x)\n",
        "\n",
        "        # Reshape back to (B, T, C, H, W)\n",
        "        x = x.reshape(B, T, H, W, C).permute(0, 1, 4, 2, 3)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BbJy4gp5gj47"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialResizing(nn.Module):\n",
        "  def __init__(self, channels, transpose=False):\n",
        "    super().__init__()\n",
        "\n",
        "    if not transpose:\n",
        "      self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "    else:\n",
        "      self.conv = nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, t, c, h, w = x.shape\n",
        "\n",
        "    out = []\n",
        "    for i in range(t):\n",
        "      out.append(self.conv(x[:, i]))\n",
        "\n",
        "    return torch.stack(out, dim=1)"
      ],
      "metadata": {
        "id": "Lw46V-wwSl_u"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalResizing(nn.Module):\n",
        "    def __init__(self, time_dim, imgs_dims, transpose=False):\n",
        "        super().__init__()\n",
        "        self.transpose = transpose\n",
        "        self.time_dim = time_dim\n",
        "        self.conv_channels = imgs_dims[0] * imgs_dims[1] * imgs_dims[2]\n",
        "\n",
        "        if not transpose:\n",
        "            self.conv = nn.Conv1d(self.conv_channels, self.conv_channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.conv = nn.ConvTranspose1d(self.conv_channels, self.conv_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, c, h, w = x.shape\n",
        "\n",
        "        x = x.reshape(b, t, -1).permute(0, 2, 1)\n",
        "        x = self.conv(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        new_time = x.shape[1]\n",
        "        x = x.view(b, new_time, c, h, w)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "3tGeOPkFTIW3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiDimensionResizing(nn.Module):\n",
        "  def __init__(self, channels, time_dim, imgs_dims, transpose=False):\n",
        "    super().__init__()\n",
        "    self.spatial_resize = SpatialResizing(channels, transpose)\n",
        "\n",
        "    if not transpose:\n",
        "      self.imgs_dims = tuple()\n",
        "      self.imgs_dims += (channels, imgs_dims[0] // 2, imgs_dims[1] // 2)\n",
        "    else:\n",
        "      self.imgs_dims = tuple()\n",
        "      self.imgs_dims += (channels, imgs_dims[0] * 2, imgs_dims[1] * 2)\n",
        "\n",
        "    self.temporal_resize = TemporalResizing(time_dim, self.imgs_dims, transpose)\n",
        "\n",
        "  def forward(self, x):\n",
        "    spatial = self.spatial_resize(x)\n",
        "    return self.temporal_resize(spatial)"
      ],
      "metadata": {
        "id": "ZB1a1pz8TQuk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size x Time x Colors x H x W\n",
        "valid = torch.rand((64, 8, 64, 16, 16))"
      ],
      "metadata": {
        "id": "JrsO1r5IiCsL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_inf = ConvolutionBasedInflationBlock(64)\n",
        "x = conv_inf(valid)\n",
        "\n",
        "print(\"Conv\", x.shape)\n",
        "\n",
        "down = MultiDimensionResizing(64, 4, (8, 8), False)\n",
        "x = down(valid)\n",
        "\n",
        "print(\"Down\", x.shape)\n",
        "\n",
        "inflation = AttentionBasedInflationBlock(64, num_heads=8)\n",
        "x = inflation(x)\n",
        "\n",
        "print(\"Inflation\", x.shape)\n",
        "\n",
        "up = MultiDimensionResizing(64, 2,  (4, 4), True)\n",
        "x = up(x)\n",
        "\n",
        "print(\"Up\", x.shape)\n",
        "\n",
        "conv_inf_2 = ConvolutionBasedInflationBlock(64)\n",
        "x = conv_inf_2(x)\n",
        "\n",
        "print(\"Conv\", x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67VU-e7PfZxm",
        "outputId": "0db3d957-631b-4c8f-e245-e697d4e6e2e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv torch.Size([64, 4, 64, 8, 8])\n",
            "Down torch.Size([64, 2, 64, 4, 4])\n",
            "Inflation torch.Size([64, 2, 64, 4, 4])\n",
            "Up torch.Size([64, 4, 64, 8, 8])\n",
            "Conv torch.Size([64, 4, 64, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Guuw0Ux1hk3p"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}